{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Engenharia de Dados com Apache Spark","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o oficial do projeto de Engenharia de Dados com Apache Spark. Este projeto demonstra a implementa\u00e7\u00e3o de solu\u00e7\u00f5es de processamento de dados utilizando tecnologias modernas de big data.</p>"},{"location":"#visao-geral-do-projeto","title":"Vis\u00e3o Geral do Projeto","text":"<p>Este projeto demonstra como utilizar o Apache Spark em conjunto com Delta Lake e Apache Iceberg para criar, manipular e gerenciar dados de forma eficiente, confi\u00e1vel e escal\u00e1vel. A combina\u00e7\u00e3o dessas tecnologias proporciona recursos avan\u00e7ados como:</p> <ul> <li>Transa\u00e7\u00f5es ACID em dados armazenados em data lakes</li> <li>Controle de vers\u00e3o e viagem no tempo (time travel)</li> <li>Upserts e merges eficientes</li> <li>Evolu\u00e7\u00e3o de esquema e compacta\u00e7\u00e3o de arquivos</li> <li>Integra\u00e7\u00e3o perfeita com o ecossistema Spark</li> <li>Diferentes abordagens para gerenciamento de tabelas em data lakes</li> </ul> <p>Sobre o Delta Lake</p> <p>O Delta Lake \u00e9 uma camada de armazenamento de c\u00f3digo aberto que traz confiabilidade ao seu data lake. Ele fornece transa\u00e7\u00f5es ACID, manipula\u00e7\u00e3o de metadados escal\u00e1vel e unifica processamento de dados em lote e streaming.</p> <p>Sobre o Apache Iceberg</p> <p>O Apache Iceberg \u00e9 um formato de tabela aberta para conjuntos de dados anal\u00edticos de grande escala. Ele oferece controle de vers\u00e3o, evolu\u00e7\u00e3o de esquema, opera\u00e7\u00f5es transacionais ACID e particionamento oculto.</p>"},{"location":"#por-que-usar-apache-spark-com-delta-lake-e-apache-iceberg","title":"Por que usar Apache Spark com Delta Lake e Apache Iceberg?","text":"<p>O Apache Spark \u00e9 uma das ferramentas mais populares para processamento de big data, oferecendo:</p> <ul> <li>Velocidade: Execu\u00e7\u00e3o r\u00e1pida de jobs tanto em mem\u00f3ria quanto em disco</li> <li>Facilidade de uso: APIs amig\u00e1veis em Python, Java e Scala</li> <li>Generalidade: Suporte a v\u00e1rias cargas de trabalho como SQL, streaming, machine learning e processamento de grafos</li> <li>Processamento unificado: Uma \u00fanica engine para diversos casos de uso</li> </ul> <p>Quando combinado com formatos de tabela modernos como Delta Lake e Apache Iceberg, o Spark ganha recursos adicionais que resolvem muitos desafios comuns em data lakes tradicionais:</p>"},{"location":"#beneficios-do-delta-lake","title":"Benef\u00edcios do Delta Lake:","text":"<ul> <li>Confiabilidade: Transa\u00e7\u00f5es ACID garantem consist\u00eancia de dados mesmo em caso de falhas</li> <li>Qualidade de dados: Restri\u00e7\u00f5es de esquema evitam problemas com dados mal-formados</li> <li>Auditoria: Hist\u00f3rico completo de altera\u00e7\u00f5es nos dados</li> <li>Performance: Otimiza\u00e7\u00f5es espec\u00edficas para consultas e ingest\u00f5es de dados</li> </ul>"},{"location":"#beneficios-do-apache-iceberg","title":"Benef\u00edcios do Apache Iceberg:","text":"<ul> <li>Formato aberto: Projeto Apache de c\u00f3digo aberto com ampla ado\u00e7\u00e3o na ind\u00fastria</li> <li>Particionamento oculto: O particionamento \u00e9 gerenciado automaticamente pelo Iceberg</li> <li>Evolu\u00e7\u00e3o de esquema: Suporte robusto para altera\u00e7\u00f5es de esquema sem comprometer dados existentes</li> <li>SQL nativo: Opera\u00e7\u00f5es avan\u00e7adas dispon\u00edveis diretamente via SQL</li> <li>Performance: Otimiza\u00e7\u00f5es espec\u00edficas para consultas e ingest\u00f5es de dados</li> </ul>"},{"location":"#estrutura-da-documentacao","title":"Estrutura da Documenta\u00e7\u00e3o","text":"<p>Esta documenta\u00e7\u00e3o est\u00e1 organizada nas seguintes se\u00e7\u00f5es:</p> <ol> <li>Guia de Instala\u00e7\u00e3o - Instru\u00e7\u00f5es detalhadas para configurar o ambiente</li> <li>Funcionalidades - Descri\u00e7\u00e3o das principais funcionalidades implementadas</li> <li>Exemplos - Exemplos pr\u00e1ticos de uso do projeto</li> <li>Sobre - Informa\u00e7\u00f5es sobre o projeto e seus autores</li> </ol>"},{"location":"#como-comecar","title":"Como Come\u00e7ar","text":"<p>Para come\u00e7ar a utilizar o projeto, siga para o Guia de Instala\u00e7\u00e3o e configure seu ambiente de desenvolvimento.</p>"},{"location":"exemplos/","title":"Exemplos Pr\u00e1ticos","text":"<p>Esta se\u00e7\u00e3o apresenta exemplos pr\u00e1ticos implementados neste projeto, demonstrando o uso do Apache Spark com Delta Lake e Apache Iceberg para gerenciamento de dados.</p>"},{"location":"exemplos/#exemplo-com-delta-lake","title":"Exemplo com Delta Lake","text":"<p>Este exemplo demonstra as opera\u00e7\u00f5es b\u00e1sicas com o Delta Lake, conforme implementado no notebook <code>pyspark-delta/delta.ipynb</code>.</p>"},{"location":"exemplos/#configuracao-do-ambiente-spark-com-delta-lake","title":"Configura\u00e7\u00e3o do Ambiente Spark com Delta Lake","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType\nfrom delta import *\n\nspark = ( \n    SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate() \n)\n</code></pre>"},{"location":"exemplos/#criacao-do-dataframe-com-dados-de-clientes","title":"Cria\u00e7\u00e3o do DataFrame com Dados de Clientes","text":"<pre><code>data = [\n    (\"ID001\", \"CLIENTE_X\",\"SP\",\"ATIVO\",   250000.00),\n    (\"ID002\", \"CLIENTE_Y\",\"SC\",\"INATIVO\", 400000.00),\n    (\"ID003\", \"CLIENTE_Z\",\"DF\",\"ATIVO\",   1000000.00)\n]\n\nschema = (\n    StructType([\n        StructField(\"ID_CLIENTE\",     StringType(),True),\n        StructField(\"NOME_CLIENTE\",   StringType(),True),\n        StructField(\"UF\",             StringType(),True),\n        StructField(\"STATUS\",         StringType(),True),\n        StructField(\"LIMITE_CREDITO\", FloatType(), True)\n    ])\n)\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.show(truncate=False)\n</code></pre>"},{"location":"exemplos/#criacao-da-tabela-delta","title":"Cria\u00e7\u00e3o da Tabela Delta","text":"<pre><code>( \n    df\n    .write\n    .format(\"delta\")\n    .mode('overwrite')\n    .save(\"./RAW/CLIENTES\")\n)\n</code></pre>"},{"location":"exemplos/#operacoes-de-insertupdate-usando-merge","title":"Opera\u00e7\u00f5es de Insert/Update usando Merge","text":"<pre><code># Dados para merge (atualiza\u00e7\u00e3o e inser\u00e7\u00e3o)\nnew_data = [\n    (\"ID001\",\"CLIENTE_X\",\"SP\",\"INATIVO\", 0.00),       # Atualiza\u00e7\u00e3o - cliente existente\n    (\"ID002\",\"CLIENTE_Y\",\"SC\",\"ATIVO\",   400000.00),  # Atualiza\u00e7\u00e3o - cliente existente\n    (\"ID004\",\"CLIENTE_Z\",\"DF\",\"ATIVO\",   5000000.00)  # Inser\u00e7\u00e3o - novo cliente\n]\n\n# Criar DataFrame com os novos dados\ndf_new = spark.createDataFrame(data=new_data, schema=schema)\n\n# Obter refer\u00eancia \u00e0 tabela Delta\ndeltaTable = DeltaTable.forPath(spark, \"./RAW/CLIENTES\")\n\n# Executar opera\u00e7\u00e3o de merge\n(\n    deltaTable.alias(\"dados_atuais\")\n    .merge(\n        df_new.alias(\"novos_dados\"),\n        \"dados_atuais.ID_CLIENTE = novos_dados.ID_CLIENTE\"\n    )\n    .whenMatchedUpdateAll()    # Atualiza todos os campos quando encontra correspond\u00eancia\n    .whenNotMatchedInsertAll() # Insere novo registro quando n\u00e3o encontra correspond\u00eancia\n    .execute()\n)\n</code></pre>"},{"location":"exemplos/#operacao-de-delete","title":"Opera\u00e7\u00e3o de Delete","text":"<pre><code># Deletar registros com condi\u00e7\u00e3o (limite de cr\u00e9dito menor que 400000.0)\ndeltaTable.delete(\"LIMITE_CREDITO &lt; 400000.0\")\n</code></pre>"},{"location":"exemplos/#exemplo-com-apache-iceberg","title":"Exemplo com Apache Iceberg","text":"<p>Este exemplo demonstra as opera\u00e7\u00f5es b\u00e1sicas com o Apache Iceberg, conforme implementado no notebook <code>pyspark-iceberg/pyspark-iceberg.ipynb</code>.</p>"},{"location":"exemplos/#configuracao-do-ambiente-spark-com-apache-iceberg","title":"Configura\u00e7\u00e3o do Ambiente Spark com Apache Iceberg","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder\n    .appName(\"IcebergExample\")\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.0\")\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"./warehouse\")\n    .getOrCreate()\n)\n</code></pre>"},{"location":"exemplos/#criacao-do-dataframe-com-dados-de-clientes_1","title":"Cria\u00e7\u00e3o do DataFrame com Dados de Clientes","text":"<pre><code>from pyspark.sql.types import StructType, StructField, StringType, FloatType\n\ndata = [\n    (\"ID001\", \"CLIENTE_X\", \"SP\", \"ATIVO\",   250000.00),\n    (\"ID002\", \"CLIENTE_Y\", \"SC\", \"INATIVO\", 400000.00),\n    (\"ID003\", \"CLIENTE_Z\", \"DF\", \"ATIVO\",   1000000.00)\n]\n\nschema = StructType([\n    StructField(\"ID_CLIENTE\", StringType(), True),\n    StructField(\"NOME_CLIENTE\", StringType(), True),\n    StructField(\"UF\", StringType(), True),\n    StructField(\"STATUS\", StringType(), True),\n    StructField(\"LIMITE_CREDITO\", FloatType(), True)\n])\n\ndf = spark.createDataFrame(data=data, schema=schema)\ndf.show()\n</code></pre>"},{"location":"exemplos/#criacao-da-tabela-iceberg","title":"Cria\u00e7\u00e3o da Tabela Iceberg","text":"<pre><code># Criar tabela Iceberg usando a API DataFrame\ndf.writeTo(\"spark_catalog.default.clientes_iceberg\").using(\"iceberg\").createOrReplace()\n</code></pre>"},{"location":"exemplos/#operacao-de-insert-append","title":"Opera\u00e7\u00e3o de Insert (Append)","text":"<pre><code># Dados para inser\u00e7\u00e3o\nnew_data = [\n    (\"ID004\", \"CLIENTE_NEW\", \"RJ\", \"ATIVO\", 999999.00)\n]\n\n# Criar DataFrame com os novos dados\ndf_new = spark.createDataFrame(data=new_data, schema=schema)\n\n# Inserir os novos dados na tabela Iceberg\ndf_new.writeTo(\"spark_catalog.default.clientes_iceberg\").append()\n</code></pre>"},{"location":"exemplos/#operacao-de-update-usando-sql","title":"Opera\u00e7\u00e3o de Update usando SQL","text":"<pre><code># Atualizar registros usando SQL\nspark.sql(\"\"\"\n    UPDATE spark_catalog.default.clientes_iceberg\n    SET STATUS = 'INATIVO', LIMITE_CREDITO = 0.00\n    WHERE ID_CLIENTE = 'ID001'\n\"\"\")\n</code></pre>"},{"location":"exemplos/#operacao-de-delete-usando-sql","title":"Opera\u00e7\u00e3o de Delete usando SQL","text":"<pre><code># Excluir registros usando SQL\nspark.sql(\"\"\"\n    DELETE FROM spark_catalog.default.clientes_iceberg\n    WHERE LIMITE_CREDITO &lt; 400000.0\n\"\"\")\n</code></pre>"},{"location":"exemplos/#operacao-de-consulta-usando-sql","title":"Opera\u00e7\u00e3o de Consulta usando SQL","text":"<pre><code># Consultar dados da tabela Iceberg\nspark.sql(\"SELECT * FROM spark_catalog.default.clientes_iceberg\").show()\n</code></pre>"},{"location":"funcionalidades/","title":"Funcionalidades","text":"<p>Este documento detalha as principais funcionalidades implementadas no projeto de Engenharia de Dados com Apache Spark, com foco especial nas opera\u00e7\u00f5es utilizando Delta Lake.</p>"},{"location":"funcionalidades/#visao-geral-das-operacoes-com-delta-lake","title":"Vis\u00e3o Geral das Opera\u00e7\u00f5es com Delta Lake","text":"<p>O Delta Lake \u00e9 uma camada de armazenamento que adiciona recursos essenciais aos data lakes, trazendo confiabilidade e performance para opera\u00e7\u00f5es de dados em larga escala. Este projeto demonstra a implementa\u00e7\u00e3o das seguintes capacidades:</p> <ul> <li>ACID Transactions: Garantia de atomicidade, consist\u00eancia, isolamento e durabilidade para todas as opera\u00e7\u00f5es de dados</li> <li>Schema Enforcement: Preven\u00e7\u00e3o autom\u00e1tica contra inser\u00e7\u00e3o de dados inv\u00e1lidos</li> <li>Schema Evolution: Modifica\u00e7\u00e3o flex\u00edvel do esquema ao longo do tempo</li> <li>Time Travel: Acesso a vers\u00f5es anteriores dos dados</li> <li>CRUD Operations: Suporte completo a Create, Read, Update e Delete </li> <li>Merge Operations: Opera\u00e7\u00f5es de UPSERT eficientes (update + insert)</li> <li>Data Optimization: Compacta\u00e7\u00e3o de arquivos e otimiza\u00e7\u00e3o de consultas</li> </ul>"},{"location":"funcionalidades/#operacoes-crud-com-delta-lake","title":"Opera\u00e7\u00f5es CRUD com Delta Lake","text":""},{"location":"funcionalidades/#1-criacao-de-tabelas-delta-create","title":"1. Cria\u00e7\u00e3o de Tabelas Delta (Create)","text":"<p>O Delta Lake permite criar tabelas a partir de DataFrames Spark com sintaxe simples, adicionando garantias de transa\u00e7\u00e3o e capacidades avan\u00e7adas.</p>"},{"location":"funcionalidades/#exemplo-de-criacao","title":"Exemplo de Cria\u00e7\u00e3o:","text":"<pre><code># Importa\u00e7\u00f5es necess\u00e1rias\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType\nfrom delta import *\n\n# Configura\u00e7\u00e3o da sess\u00e3o Spark com suporte ao Delta Lake\nspark = (\n    SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n\n# Dados de exemplo\ndata = [\n    (\"ID001\", \"CLIENTE_X\", \"SP\", \"ATIVO\",   250000.00),\n    (\"ID002\", \"CLIENTE_Y\", \"SC\", \"INATIVO\", 400000.00),\n    (\"ID003\", \"CLIENTE_Z\", \"DF\", \"ATIVO\",   1000000.00)\n]\n\n# Defini\u00e7\u00e3o do schema\nschema = (\n    StructType([\n        StructField(\"ID_CLIENTE\",     StringType(), True),\n        StructField(\"NOME_CLIENTE\",   StringType(), True),\n        StructField(\"UF\",             StringType(), True),\n        StructField(\"STATUS\",         StringType(), True),\n        StructField(\"LIMITE_CREDITO\", FloatType(),  True)\n    ])\n)\n\n# Cria\u00e7\u00e3o do DataFrame\ndf = spark.createDataFrame(data=data, schema=schema)\n\n# Escrita no formato Delta\n(\n    df\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")  # Op\u00e7\u00f5es: append, overwrite, ignore, errorIfExists\n    .save(\"./RAW/CLIENTES\")\n)\n</code></pre> <p>Modos de Escrita</p> <p>O Delta Lake suporta diferentes modos de escrita: - overwrite: Substitui dados existentes - append: Adiciona novos dados sem modificar existentes - ignore: N\u00e3o faz nada se o destino j\u00e1 existir - errorIfExists: Falha se o destino j\u00e1 existir</p>"},{"location":"funcionalidades/#2-leitura-de-tabelas-delta-read","title":"2. Leitura de Tabelas Delta (Read)","text":"<p>A leitura de tabelas Delta \u00e9 feita com a mesma API do Spark, mas com benef\u00edcios adicionais como consist\u00eancia de leitura e capacidade de acessar vers\u00f5es anteriores.</p>"},{"location":"funcionalidades/#exemplo-de-leitura","title":"Exemplo de Leitura:","text":"<pre><code># Leitura simples de tabela Delta\ndf_clientes = (\n    spark\n    .read\n    .format(\"delta\")\n    .load(\"./RAW/CLIENTES\")\n)\n\n# Exibir resultados\ndf_clientes.show()\n</code></pre>"},{"location":"funcionalidades/#leitura-de-versao-especifica-time-travel","title":"Leitura de Vers\u00e3o Espec\u00edfica (Time Travel):","text":"<pre><code># Ler vers\u00e3o espec\u00edfica da tabela\ndf_clientes_versao_anterior = (\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"versionAsOf\", 0)  # Vers\u00e3o 0 (inicial)\n    .load(\"./RAW/CLIENTES\")\n)\n\n# Alternativa usando timestamp\ndf_clientes_timestamp = (\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"timestampAsOf\", \"2023-01-01 00:00:00\")\n    .load(\"./RAW/CLIENTES\")\n)\n</code></pre>"},{"location":"funcionalidades/#3-atualizacao-de-dados-update","title":"3. Atualiza\u00e7\u00e3o de Dados (Update)","text":"<p>O Delta Lake suporta atualiza\u00e7\u00f5es de dados de forma eficiente, permitindo modificar registros existentes sem reescrever toda a tabela.</p>"},{"location":"funcionalidades/#exemplo-de-atualizacao","title":"Exemplo de Atualiza\u00e7\u00e3o:","text":"<pre><code>from delta.tables import DeltaTable\n\n# Carregar tabela Delta para opera\u00e7\u00f5es\ndeltaTable = DeltaTable.forPath(spark, \"./RAW/CLIENTES\")\n\n# Atualizar registros\n(\n    deltaTable.update(\n        condition=\"STATUS = 'ATIVO'\",\n        set={\"LIMITE_CREDITO\": \"LIMITE_CREDITO * 1.1\"}  # Aumento de 10% no limite\n    )\n)\n</code></pre>"},{"location":"funcionalidades/#4-exclusao-de-dados-delete","title":"4. Exclus\u00e3o de Dados (Delete)","text":"<p>Opera\u00e7\u00f5es de exclus\u00e3o permitem remover registros que atendem a determinadas condi\u00e7\u00f5es.</p>"},{"location":"funcionalidades/#exemplo-de-exclusao","title":"Exemplo de Exclus\u00e3o:","text":"<pre><code># Deletar registros com condi\u00e7\u00e3o\ndeltaTable.delete(\"LIMITE_CREDITO &lt; 400000.0\")\n</code></pre>"},{"location":"funcionalidades/#merge-operations-upsert","title":"Merge Operations (UPSERT)","text":"<p>Uma das opera\u00e7\u00f5es mais poderosas do Delta Lake \u00e9 o <code>merge</code>, que permite combinar opera\u00e7\u00f5es de atualiza\u00e7\u00e3o e inser\u00e7\u00e3o em uma \u00fanica transa\u00e7\u00e3o at\u00f4mica.</p>"},{"location":"funcionalidades/#exemplo-de-merge","title":"Exemplo de MERGE:","text":"<pre><code># Novos dados para merge\nnew_data = [\n    (\"ID001\", \"CLIENTE_X\", \"SP\", \"INATIVO\", 0.00),         # Update - cliente existente\n    (\"ID002\", \"CLIENTE_Y\", \"SC\", \"ATIVO\",   400000.00),    # Update - cliente existente\n    (\"ID004\", \"CLIENTE_Z\", \"DF\", \"ATIVO\",   5000000.00)    # Insert - novo cliente\n]\n\n# Criar DataFrame com novos dados\ndf_new = spark.createDataFrame(data=new_data, schema=schema)\n\n# Carregar tabela Delta\ndeltaTable = DeltaTable.forPath(spark, \"./RAW/CLIENTES\")\n\n# Executar opera\u00e7\u00e3o de merge\n(\n    deltaTable.alias(\"dados_atuais\")\n    .merge(\n        df_new.alias(\"novos_dados\"),\n        \"dados_atuais.ID_CLIENTE = novos_dados.ID_CLIENTE\"  # Condi\u00e7\u00e3o de merge\n    )\n    .whenMatchedUpdateAll()      # Atualiza todos os campos quando encontra correspond\u00eancia\n    .whenNotMatchedInsertAll()   # Insere novo registro quando n\u00e3o encontra correspond\u00eancia\n    .execute()\n)\n</code></pre> <p>Controle Granular</p> <p>Voc\u00ea tamb\u00e9m pode ter controle mais granular usando: <pre><code>.whenMatched(condition=\"novos_dados.LIMITE_CREDITO &gt; 0\")\n.updateExpr({\"STATUS\": \"novos_dados.STATUS\", \"LIMITE_CREDITO\": \"novos_dados.LIMITE_CREDITO\"})\n.whenNotMatched(condition=\"novos_dados.LIMITE_CREDITO &gt; 1000000\")\n.insertExpr({\n    \"ID_CLIENTE\": \"novos_dados.ID_CLIENTE\",\n    \"NOME_CLIENTE\": \"novos_dados.NOME_CLIENTE\",\n    \"UF\": \"novos_dados.UF\",\n    \"STATUS\": \"novos_dados.STATUS\",\n    \"LIMITE_CREDITO\": \"novos_dados.LIMITE_CREDITO\"\n})\n</code></pre></p>"},{"location":"funcionalidades/#recursos-avancados","title":"Recursos Avan\u00e7ados","text":""},{"location":"funcionalidades/#1-time-travel-viagem-no-tempo","title":"1. Time Travel (Viagem no Tempo)","text":"<p>O Delta Lake mant\u00e9m um hist\u00f3rico de todas as modifica\u00e7\u00f5es, permitindo acessar e consultar vers\u00f5es anteriores dos dados.</p>"},{"location":"funcionalidades/#exemplos-de-time-travel","title":"Exemplos de Time Travel:","text":"<pre><code># Listar hist\u00f3rico de vers\u00f5es\ndeltaTable.history().show()\n\n# Ler vers\u00e3o espec\u00edfica\ndf_versao_1 = (\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"versionAsOf\", 1)\n    .load(\"./RAW/CLIENTES\")\n)\n\n# Restaurar para vers\u00e3o anterior\n(\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"versionAsOf\", 1)\n    .load(\"./RAW/CLIENTES\")\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .save(\"./RAW/CLIENTES\")\n)\n</code></pre>"},{"location":"funcionalidades/#2-otimizacoes-de-dados","title":"2. Otimiza\u00e7\u00f5es de Dados","text":"<p>O Delta Lake oferece comandos para otimizar o layout dos arquivos e melhorar o desempenho das consultas.</p>"},{"location":"funcionalidades/#compactacao-de-arquivos","title":"Compacta\u00e7\u00e3o de Arquivos:","text":"<pre><code># Compactar arquivos pequenos\nspark.sql(f\"OPTIMIZE delta.`./RAW/CLIENTES`\")\n\n# Compactar e ordenar por coluna espec\u00edfica (Z-Order)\nspark.sql(f\"OPTIMIZE delta.`./RAW/CLIENTES` ZORDER BY (UF, STATUS)\")\n</code></pre>"},{"location":"funcionalidades/#vacuum-limpeza-de-arquivos-antigos","title":"Vacuum (Limpeza de Arquivos Antigos):","text":"<pre><code># Remover arquivos antigos que n\u00e3o s\u00e3o mais necess\u00e1rios para time travel\n# Mantendo apenas 7 dias de hist\u00f3rico\ndeltaTable.vacuum(retentionHours=24*7)\n</code></pre> <p>Cuidado com Vacuum</p> <p>O comando <code>vacuum</code> remove permanentemente arquivos antigos, limitando o time travel. Por padr\u00e3o, o Delta Lake impede a remo\u00e7\u00e3o de arquivos com menos de 7 dias para evitar problemas com consultas ativas.</p>"},{"location":"funcionalidades/#3-schema-evolution-evolucao-de-esquema","title":"3. Schema Evolution (Evolu\u00e7\u00e3o de Esquema)","text":"<p>O Delta Lake permite adicionar, remover ou modificar colunas de maneira controlada.</p>"},{"location":"funcionalidades/#exemplo-de-evolucao-de-esquema","title":"Exemplo de Evolu\u00e7\u00e3o de Esquema:","text":"<pre><code># Adicionar nova coluna ao esquema\nfrom pyspark.sql.functions import lit\n\n# Ler dados existentes\ndf_atual = spark.read.format(\"delta\").load(\"./RAW/CLIENTES\")\n\n# Adicionar nova coluna\ndf_com_nova_coluna = df_atual.withColumn(\"DATA_ATUALIZACAO\", lit(\"2024-04-23\"))\n\n# Escrever de volta com esquema atualizado\n(\n    df_com_nova_coluna\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"mergeSchema\", \"true\")  # Habilita evolu\u00e7\u00e3o de esquema\n    .save(\"./RAW/CLIENTES\")\n)\n</code></pre>"},{"location":"funcionalidades/#apache-iceberg","title":"Apache Iceberg","text":"<p>O Apache Iceberg \u00e9 um formato de tabela aberta para conjuntos de dados anal\u00edticos de grande escala. Assim como o Delta Lake, o Iceberg oferece transa\u00e7\u00f5es ACID, controle de vers\u00e3o de dados, e evolu\u00e7\u00e3o de esquema, mas com algumas diferen\u00e7as em sua implementa\u00e7\u00e3o e recursos.</p>"},{"location":"funcionalidades/#visao-geral-do-apache-iceberg","title":"Vis\u00e3o Geral do Apache Iceberg","text":"<p>O Iceberg foi projetado para resolver os problemas comuns encontrados em data lakes, oferecendo:</p> <ul> <li>Transa\u00e7\u00f5es ACID: Garantia de atomicidade, consist\u00eancia, isolamento e durabilidade</li> <li>Schema Evolution: Suporte para mudan\u00e7as de esquema sem comprometer dados existentes</li> <li>Time Travel: Acesso a vers\u00f5es anteriores dos dados (hist\u00f3rico)</li> <li>Particionamento Oculto: O particionamento \u00e9 gerenciado pelo Iceberg, sem exigir que os usu\u00e1rios conhe\u00e7am os detalhes</li> <li>Compacta\u00e7\u00e3o de Arquivos: Otimiza\u00e7\u00e3o autom\u00e1tica de arquivos pequenos</li> <li>Formatos de Dados Abertos: Suporte para formatos como Parquet, Avro e ORC</li> </ul>"},{"location":"funcionalidades/#configuracao-do-apache-iceberg-com-pyspark","title":"Configura\u00e7\u00e3o do Apache Iceberg com PySpark","text":"<p>Para utilizar o Apache Iceberg com PySpark, \u00e9 necess\u00e1rio configurar a sess\u00e3o Spark adequadamente:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder\n    .appName(\"IcebergExample\")\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.0\")\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"./warehouse\")\n    .getOrCreate()\n)\n</code></pre> <p>Depend\u00eancias do Iceberg</p> <p>O Iceberg requer depend\u00eancias espec\u00edficas baseadas na vers\u00e3o do Spark. Para o Spark 3.4, utilizamos <code>iceberg-spark-runtime-3.4_2.12:1.3.0</code>.</p>"},{"location":"funcionalidades/#operacoes-crud-com-apache-iceberg","title":"Opera\u00e7\u00f5es CRUD com Apache Iceberg","text":""},{"location":"funcionalidades/#1-criacao-de-tabelas-iceberg-create","title":"1. Cria\u00e7\u00e3o de Tabelas Iceberg (Create)","text":"<p>A cria\u00e7\u00e3o de tabelas Iceberg \u00e9 semelhante \u00e0 cria\u00e7\u00e3o de tabelas Delta, mas com a API espec\u00edfica do Iceberg:</p> <pre><code>from pyspark.sql.types import StructType, StructField, StringType, FloatType\n\n# Dados de exemplo\ndata = [\n    (\"ID001\", \"CLIENTE_X\", \"SP\", \"ATIVO\",   250000.00),\n    (\"ID002\", \"CLIENTE_Y\", \"SC\", \"INATIVO\", 400000.00),\n    (\"ID003\", \"CLIENTE_Z\", \"DF\", \"ATIVO\",   1000000.00)\n]\n\n# Defini\u00e7\u00e3o do schema\nschema = StructType([\n    StructField(\"ID_CLIENTE\", StringType(), True),\n    StructField(\"NOME_CLIENTE\", StringType(), True),\n    StructField(\"UF\", StringType(), True),\n    StructField(\"STATUS\", StringType(), True),\n    StructField(\"LIMITE_CREDITO\", FloatType(), True)\n])\n\n# Cria\u00e7\u00e3o do DataFrame\ndf = spark.createDataFrame(data=data, schema=schema)\n\n# Criar uma tabela Iceberg\ndf.writeTo(\"spark_catalog.default.clientes_iceberg\").using(\"iceberg\").createOrReplace()\n</code></pre>"},{"location":"funcionalidades/#2-insercao-de-dados-insert","title":"2. Inser\u00e7\u00e3o de Dados (Insert)","text":"<p>O Iceberg permite adicionar novos registros a uma tabela existente:</p> <pre><code># Novos dados para inser\u00e7\u00e3o\nnew_data = [\n    (\"ID004\", \"CLIENTE_NEW\", \"RJ\", \"ATIVO\", 999999.00)\n]\n\n# Criar DataFrame com novos dados\ndf_new = spark.createDataFrame(data=new_data, schema=schema)\n\n# Inserir dados na tabela Iceberg existente\ndf_new.writeTo(\"spark_catalog.default.clientes_iceberg\").append()\n</code></pre>"},{"location":"funcionalidades/#3-atualizacao-de-dados-update_1","title":"3. Atualiza\u00e7\u00e3o de Dados (Update)","text":"<p>O Iceberg suporta atualiza\u00e7\u00f5es SQL para modificar registros existentes:</p> <pre><code># Atualizar registros com SQL\nspark.sql(\"\"\"\n    UPDATE spark_catalog.default.clientes_iceberg\n    SET STATUS = 'INATIVO', LIMITE_CREDITO = 0.00\n    WHERE ID_CLIENTE = 'ID001'\n\"\"\")\n</code></pre>"},{"location":"funcionalidades/#4-exclusao-de-dados-delete_1","title":"4. Exclus\u00e3o de Dados (Delete)","text":"<p>Exclus\u00f5es podem ser realizadas com express\u00f5es SQL:</p> <pre><code># Excluir registros com SQL\nspark.sql(\"\"\"\n    DELETE FROM spark_catalog.default.clientes_iceberg\n    WHERE LIMITE_CREDITO &lt; 400000.0\n\"\"\")\n</code></pre>"},{"location":"funcionalidades/#5-consulta-de-dados-query","title":"5. Consulta de Dados (Query)","text":"<p>Consultar dados de tabelas Iceberg \u00e9 feito com SQL padr\u00e3o:</p> <pre><code># Consultar todos os registros\nspark.sql(\"SELECT * FROM spark_catalog.default.clientes_iceberg\").show()\n</code></pre>"},{"location":"funcionalidades/#recursos-avancados-do-apache-iceberg","title":"Recursos Avan\u00e7ados do Apache Iceberg","text":""},{"location":"funcionalidades/#1-time-travel","title":"1. Time Travel","text":"<p>O Iceberg mant\u00e9m o hist\u00f3rico de transa\u00e7\u00f5es, permitindo acessar vers\u00f5es anteriores dos dados:</p> <pre><code># Consultar uma vers\u00e3o espec\u00edfica da tabela por ID de snapshot\nspark.read.format(\"iceberg\").option(\"snapshot-id\", \"8485094958438453408\").load(\"spark_catalog.default.clientes_iceberg\")\n\n# Consultar por timestamp\nspark.read.format(\"iceberg\").option(\"as-of-timestamp\", \"2023-04-23 10:00:00\").load(\"spark_catalog.default.clientes_iceberg\")\n</code></pre>"},{"location":"funcionalidades/#2-evolucao-de-esquema-schema-evolution","title":"2. Evolu\u00e7\u00e3o de Esquema (Schema Evolution)","text":"<p>O Iceberg suporta altera\u00e7\u00f5es de esquema como adicionar, renomear, ou remover colunas:</p> <pre><code># Adicionar uma nova coluna\nspark.sql(\"\"\"\n    ALTER TABLE spark_catalog.default.clientes_iceberg \n    ADD COLUMN data_cadastro TIMESTAMP\n\"\"\")\n\n# Renomear uma coluna\nspark.sql(\"\"\"\n    ALTER TABLE spark_catalog.default.clientes_iceberg \n    RENAME COLUMN UF TO estado\n\"\"\")\n</code></pre>"},{"location":"funcionalidades/#3-otimizacao-e-manutencao","title":"3. Otimiza\u00e7\u00e3o e Manuten\u00e7\u00e3o","text":"<p>O Iceberg fornece comandos para otimizar e manter tabelas:</p> <pre><code># Compactar arquivos pequenos\nspark.sql(\"\"\"\n    CALL spark_catalog.system.rewrite_data_files(table =&gt; 'default.clientes_iceberg')\n\"\"\")\n\n# Remover snapshots antigos\nspark.sql(\"\"\"\n    CALL spark_catalog.system.expire_snapshots(table =&gt; 'default.clientes_iceberg', \n                                             older_than =&gt; TIMESTAMP '2023-04-01 00:00:00')\n\"\"\")\n</code></pre>"},{"location":"funcionalidades/#comparacao-entre-delta-lake-e-apache-iceberg","title":"Compara\u00e7\u00e3o entre Delta Lake e Apache Iceberg","text":"Caracter\u00edstica Delta Lake Apache Iceberg Transa\u00e7\u00f5es ACID \u2713 \u2713 Time Travel \u2713 \u2713 Schema Evolution \u2713 \u2713 Compatibilidade Integrado ao Databricks Projeto Apache, ampla ado\u00e7\u00e3o Metadados Parquet + JSON Formato pr\u00f3prio (avro ou json) Particionamento Expl\u00edcito Oculto / Gerenciado Suporte para nomes de colunas Case insensitive Case sensitive Otimiza\u00e7\u00e3o Z-Order e Optimize Ordena\u00e7\u00e3o e Compacta\u00e7\u00e3o"},{"location":"funcionalidades/#integracao-com-sql","title":"Integra\u00e7\u00e3o com SQL","text":"<p>O Delta Lake se integra perfeitamente com SQL, permitindo usar sintaxe familiar para opera\u00e7\u00f5es:</p> <pre><code># Registrar tabela para uso com SQL\nspark.sql(f\"CREATE TABLE clientes USING DELTA LOCATION './RAW/CLIENTES'\")\n\n# Consultar dados com SQL\nresultado = spark.sql(\"SELECT * FROM clientes WHERE UF = 'SP'\")\nresultado.show()\n\n# Atualizar com SQL\nspark.sql(\"UPDATE clientes SET LIMITE_CREDITO = 300000 WHERE ID_CLIENTE = 'ID001'\")\n\n# Merge com SQL\nspark.sql(\"\"\"\nMERGE INTO clientes AS target\nUSING updates AS source\nON target.ID_CLIENTE = source.ID_CLIENTE\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n\"\"\")\n</code></pre>"},{"location":"funcionalidades/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Agora que voc\u00ea conhece as principais funcionalidades do Delta Lake implementadas neste projeto, explore a se\u00e7\u00e3o de Exemplos para ver casos de uso completos e aplica\u00e7\u00f5es pr\u00e1ticas destas funcionalidades.</p>"},{"location":"guia-instalacao/","title":"Guia de Instala\u00e7\u00e3o","text":"<p>Este guia fornece instru\u00e7\u00f5es detalhadas para configurar seu ambiente de desenvolvimento e instalar todas as depend\u00eancias necess\u00e1rias para trabalhar com o projeto de Engenharia de Dados com Apache Spark.</p>"},{"location":"guia-instalacao/#requisitos-do-sistema","title":"Requisitos do Sistema","text":"<p>Antes de iniciar a instala\u00e7\u00e3o, certifique-se de que seu sistema atende aos seguintes requisitos:</p>"},{"location":"guia-instalacao/#requisitos-de-hardware","title":"Requisitos de Hardware","text":"<ul> <li>Processador: M\u00ednimo de 2 cores (recomendado 4+ cores)</li> <li>Mem\u00f3ria RAM: M\u00ednimo de 4GB (recomendado 8GB+ para cargas de trabalho maiores)</li> <li>Espa\u00e7o em Disco: M\u00ednimo de 2GB dispon\u00edveis</li> </ul>"},{"location":"guia-instalacao/#pre-requisitos-de-software","title":"Pr\u00e9-requisitos de Software","text":"<ul> <li>Sistema Operacional: Windows 10/11, macOS (10.15+), ou Linux (Ubuntu 20.04+, CentOS 7+)</li> <li>Python: Vers\u00e3o 3.9.9</li> <li>Java: JDK 8 ou superior (necess\u00e1rio para Apache Spark)</li> <li>Poetry: Ferramenta de gerenciamento de depend\u00eancias para Python</li> </ul> <p>Importante sobre Java</p> <p>O Apache Spark requer Java para funcionar. Certifique-se de ter o JDK instalado e a vari\u00e1vel de ambiente <code>JAVA_HOME</code> configurada corretamente.</p>"},{"location":"guia-instalacao/#instalacao-do-python","title":"Instala\u00e7\u00e3o do Python","text":"<p>Se voc\u00ea ainda n\u00e3o tem Python 3.9.9 instalado:</p>"},{"location":"guia-instalacao/#para-windows","title":"Para Windows","text":"<ol> <li>Baixe o instalador do Python em python.org</li> <li>Execute o instalador e marque a op\u00e7\u00e3o \"Add Python to PATH\"</li> <li>Conclua a instala\u00e7\u00e3o seguindo as instru\u00e7\u00f5es na tela</li> </ol>"},{"location":"guia-instalacao/#para-macos","title":"Para macOS","text":"<p>Utilizando o Homebrew: <pre><code>brew install python@3.9\n</code></pre></p>"},{"location":"guia-instalacao/#para-linux-ubuntudebian","title":"Para Linux (Ubuntu/Debian)","text":"<pre><code>sudo apt update\nsudo apt install python3.9 python3.9-venv python3.9-dev\n</code></pre>"},{"location":"guia-instalacao/#instalacao-do-poetry","title":"Instala\u00e7\u00e3o do Poetry","text":"<p>O Poetry \u00e9 a ferramenta recomendada para gerenciar depend\u00eancias e ambientes virtuais neste projeto.</p>"},{"location":"guia-instalacao/#para-windows-macos-e-linux","title":"Para Windows, macOS e Linux","text":"<pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Ap\u00f3s a instala\u00e7\u00e3o, adicione o Poetry ao seu PATH seguindo as instru\u00e7\u00f5es exibidas no final da instala\u00e7\u00e3o.</p> <p>Para verificar a instala\u00e7\u00e3o, execute: <pre><code>poetry --version\n</code></pre></p>"},{"location":"guia-instalacao/#clonando-o-repositorio","title":"Clonando o Reposit\u00f3rio","text":"<p>Clone o reposit\u00f3rio do projeto utilizando Git:</p> <pre><code>git clone https://github.com/Matheus2037/Eng-Dados-Apache-Spark.git\ncd Eng-Dados-Apache-Spark\n</code></pre>"},{"location":"guia-instalacao/#instalando-dependencias-com-poetry","title":"Instalando Depend\u00eancias com Poetry","text":"<p>Uma vez dentro do diret\u00f3rio do projeto, instale todas as depend\u00eancias:</p> <pre><code>poetry install\n</code></pre> <p>Este comando criar\u00e1 um ambiente virtual e instalar\u00e1 as seguintes depend\u00eancias principais: - pyspark (3.4.2): Motor de processamento distribu\u00eddo - delta-spark (2.4.0): Extens\u00e3o do Spark para Delta Lake - iceberg-spark-runtime-3.4_2.12 (1.3.0): Extens\u00e3o do Spark para Apache Iceberg - jupyterlab (4.4.0+): Ambiente interativo para execu\u00e7\u00e3o de notebooks</p>"},{"location":"guia-instalacao/#ativando-o-ambiente-virtual","title":"Ativando o Ambiente Virtual","text":"<p>Para ativar o ambiente virtual criado pelo Poetry:</p> <pre><code>poetry env activate\n</code></pre> <p>Ou voc\u00ea pode executar comandos diretamente dentro do ambiente virtual sem ativ\u00e1-lo permanentemente:</p> <pre><code>poetry run jupyter lab\n</code></pre>"},{"location":"guia-instalacao/#verificando-a-instalacao","title":"Verificando a Instala\u00e7\u00e3o","text":"<p>Para verificar se tudo foi instalado corretamente:</p> <ol> <li>Ative o ambiente virtual: <code>poetry env activate</code></li> <li>Abra o Python REPL: <code>python</code></li> <li>Tente importar as bibliotecas principais:</li> </ol> <pre><code>import pyspark\nfrom delta import *\nimport importlib\n\nprint(f\"PySpark vers\u00e3o: {pyspark.__version__}\")\nprint(\"Delta Lake dispon\u00edvel!\")\n\n# Verificar disponibilidade do Apache Iceberg\niceberg_module = importlib.util.find_spec(\"pyiceberg\")\nif iceberg_module is not None:\n    print(\"Apache Iceberg dispon\u00edvel!\")\nelse:\n    print(\"Apache Iceberg n\u00e3o encontrado. Verifique a instala\u00e7\u00e3o.\")\n</code></pre> <p>Se n\u00e3o ocorrer nenhum erro, sua instala\u00e7\u00e3o est\u00e1 funcionando corretamente.</p>"},{"location":"guia-instalacao/#iniciando-o-jupyterlab","title":"Iniciando o JupyterLab","text":"<p>Para iniciar o JupyterLab e acessar os notebooks do projeto:</p> <pre><code>poetry run jupyter lab\n</code></pre> <p>Ou</p> <pre><code>jupyter-lab\n</code></pre> <p>Isso abrir\u00e1 o JupyterLab no seu navegador padr\u00e3o. Navegue at\u00e9 a pasta <code>pyspark-delta</code> para acessar os notebooks de exemplo.</p>"},{"location":"guia-instalacao/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guia-instalacao/#problema-erro-ao-instalar-o-pyspark","title":"Problema: Erro ao instalar o PySpark","text":"<p>Sintoma: Erros relacionados a depend\u00eancias do Java durante a instala\u00e7\u00e3o do PySpark.</p> <p>Solu\u00e7\u00e3o: Certifique-se de que o Java (JDK 8+) est\u00e1 instalado e configurado: <pre><code># Verifique a instala\u00e7\u00e3o do Java\njava -version\n\n# Configure a vari\u00e1vel JAVA_HOME se necess\u00e1rio\nexport JAVA_HOME=/caminho/para/seu/jdk\n</code></pre></p>"},{"location":"guia-instalacao/#problema-dependencias-nao-encontradas-apos-instalacao","title":"Problema: Depend\u00eancias n\u00e3o encontradas ap\u00f3s instala\u00e7\u00e3o","text":"<p>Sintoma: M\u00f3dulos n\u00e3o encontrados mesmo ap\u00f3s instala\u00e7\u00e3o com Poetry.</p> <p>Solu\u00e7\u00e3o: Certifique-se de que est\u00e1 usando o ambiente virtual correto: <pre><code># Verifique se o ambiente virtual est\u00e1 ativo\npoetry env info\n\n# Ative o ambiente se necess\u00e1rio\npoetry env activate\n</code></pre></p>"},{"location":"guia-instalacao/#problema-o-jupyterlab-nao-consegue-encontrar-dependencias","title":"Problema: O JupyterLab n\u00e3o consegue encontrar depend\u00eancias","text":"<p>Sintoma: Erros de importa\u00e7\u00e3o ao executar c\u00e9lulas no JupyterLab.</p> <p>Solu\u00e7\u00e3o: Certifique-se de que o Jupyter est\u00e1 usando o kernel correto: 1. No JupyterLab, verifique no canto superior direito qual kernel est\u00e1 sendo usado 2. Selecione o kernel correspondente ao seu ambiente Poetry 3. Se o kernel n\u00e3o estiver dispon\u00edvel, instale o ipykernel no ambiente:    <pre><code>poetry run pip install ipykernel\npoetry run python -m ipykernel install --user --name=eng-dados-spark\n</code></pre></p>"},{"location":"guia-instalacao/#problema-erros-relacionados-ao-delta-lake","title":"Problema: Erros relacionados ao Delta Lake","text":"<p>Sintoma: <code>ClassNotFoundException</code> ou erros similares ao tentar usar funcionalidades do Delta Lake.</p> <p>Solu\u00e7\u00e3o: Verifique se as depend\u00eancias do Delta Lake est\u00e3o corretamente configuradas: <pre><code>from pyspark.sql import SparkSession\n\n# Configure a sess\u00e3o Spark com suporte ao Delta Lake\nspark = SparkSession.builder \\\n    .appName(\"DeltaLakeTest\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n</code></pre></p>"},{"location":"guia-instalacao/#problema-erros-relacionados-ao-apache-iceberg","title":"Problema: Erros relacionados ao Apache Iceberg","text":"<p>Sintoma: <code>ClassNotFoundException</code>, <code>NoClassDefFoundError</code> ou erros similares ao tentar usar funcionalidades do Iceberg.</p> <p>Solu\u00e7\u00e3o: Verifique se as depend\u00eancias do Iceberg est\u00e3o corretamente configuradas: <pre><code>from pyspark.sql import SparkSession\n\n# Configure a sess\u00e3o Spark com suporte ao Apache Iceberg\nspark = SparkSession.builder \\\n    .appName(\"IcebergTest\") \\\n    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.0\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n    .config(\"spark.sql.catalog.local.warehouse\", \"warehouse_path\") \\\n    .getOrCreate()\n</code></pre></p>"},{"location":"guia-instalacao/#problema-tabelas-iceberg-nao-sao-reconhecidas","title":"Problema: Tabelas Iceberg n\u00e3o s\u00e3o reconhecidas","text":"<p>Sintoma: Erros do tipo <code>Table not found</code> ou tabelas Iceberg n\u00e3o podem ser lidas/escritas.</p> <p>Solu\u00e7\u00e3o: Certifique-se de que os cat\u00e1logos do Iceberg est\u00e3o configurados corretamente: <pre><code># Verifique se o cat\u00e1logo est\u00e1 configurado\nspark.sql(\"SHOW NAMESPACES\").show()\n\n# Crie um namespace se necess\u00e1rio\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS local.db\")\n\n# Certifique-se de usar o caminho correto para as tabelas\nspark.sql(\"CREATE TABLE local.db.example (id INT, data STRING) USING iceberg\")\n</code></pre></p>"},{"location":"guia-instalacao/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Ap\u00f3s completar a instala\u00e7\u00e3o, voc\u00ea est\u00e1 pronto para explorar as funcionalidades do projeto e executar os exemplos dispon\u00edveis.</p>"},{"location":"sobre/","title":"Sobre o Projeto","text":""},{"location":"sobre/#o-projeto","title":"O Projeto","text":"<p>O projeto Engenharia de Dados com Apache Spark foi desenvolvido para demonstrar a implementa\u00e7\u00e3o de solu\u00e7\u00f5es modernas de processamento de dados utilizando Apache Spark e Delta Lake. Este projeto serve como uma refer\u00eancia pr\u00e1tica para engenheiros de dados e cientistas de dados que desejam aprender e aplicar t\u00e9cnicas de processamento de big data com garantias transacionais.</p>"},{"location":"sobre/#objetivos-do-projeto","title":"Objetivos do Projeto","text":"<ul> <li>Demonstrar as capacidades do Apache Spark para processamento de dados em larga escala</li> <li>Implementar opera\u00e7\u00f5es CRUD completas utilizando Delta Lake e Apache Iceberg</li> <li>Fornecer exemplos pr\u00e1ticos de pipelines de dados e casos de uso reais</li> <li>Servir como material educativo e de refer\u00eancia para profissionais de dados</li> <li>Comparar diferentes formatos de tabela para data lakes</li> </ul>"},{"location":"sobre/#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Apache Spark: Framework de processamento distribu\u00eddo de c\u00f3digo aberto</li> <li>Delta Lake: Camada de armazenamento para confiabilidade em data lakes</li> <li>Apache Iceberg: Formato de tabela de c\u00f3digo aberto para conjuntos de dados anal\u00edticos enormes</li> <li>Python: Linguagem de programa\u00e7\u00e3o principal para implementa\u00e7\u00e3o</li> <li>JupyterLab: Ambiente interativo para explora\u00e7\u00e3o e documenta\u00e7\u00e3o</li> <li>Poetry: Gerenciamento de depend\u00eancias do projeto</li> </ul>"},{"location":"sobre/#equipe","title":"Equipe","text":"<p>O projeto foi desenvolvido por uma equipe de engenheiros de dados comprometidos com a excel\u00eancia t\u00e9cnica e o compartilhamento de conhecimento:</p>"},{"location":"sobre/#autores","title":"Autores","text":"Nome Email Papel Matheus da Silva Gastaldi matheusdasilvagastaldi@gmail.com Desenvolvedor Principal Gabriel Morona Coelho gabrielmorona0229@gmail.com Arquiteto de Dados Jo\u00e3o Carlos Rodrigues Martins joaocarlosrm2004@gmail.com Engenheiro de Dados"},{"location":"sobre/#como-contribuir","title":"Como Contribuir","text":"<p>N\u00f3s encorajamos e valorizamos contribui\u00e7\u00f5es da comunidade! Se voc\u00ea deseja contribuir com este projeto, aqui est\u00e3o algumas maneiras de come\u00e7ar:</p>"},{"location":"sobre/#reportando-problemas","title":"Reportando Problemas","text":"<p>Se voc\u00ea encontrar bugs ou tiver sugest\u00f5es de melhorias:</p> <ol> <li>Verifique se o problema j\u00e1 n\u00e3o foi reportado na se\u00e7\u00e3o de Issues do GitHub</li> <li>Abra uma nova issue com um t\u00edtulo claro e descri\u00e7\u00e3o detalhada</li> <li>Inclua passos para reproduzir o problema, comportamento esperado vs. atual, e capturas de tela se necess\u00e1rio</li> </ol>"},{"location":"sobre/#enviando-pull-requests","title":"Enviando Pull Requests","text":"<p>Para contribuir com c\u00f3digo:</p> <ol> <li>Fork este reposit\u00f3rio</li> <li>Crie uma branch para sua feature (<code>git checkout -b feature/nova-funcionalidade</code>)</li> <li>Fa\u00e7a commit das suas mudan\u00e7as (<code>git commit -m 'Adiciona nova funcionalidade'</code>)</li> <li>Push para a branch (<code>git push origin feature/nova-funcionalidade</code>)</li> <li>Abra um Pull Request</li> </ol>"},{"location":"sobre/#diretrizes-de-codigo","title":"Diretrizes de C\u00f3digo","text":"<ul> <li>Siga o estilo de c\u00f3digo PEP 8 para Python</li> <li>Escreva testes para novas funcionalidades</li> <li>Atualize a documenta\u00e7\u00e3o quando necess\u00e1rio</li> <li>Mantenha os commits organizados e com mensagens claras</li> </ul>"},{"location":"sobre/#licenca","title":"Licen\u00e7a","text":"<p>Este projeto \u00e9 disponibilizado sob a licen\u00e7a MIT. Veja o arquivo <code>LICENSE</code> para mais detalhes.</p> <pre><code>MIT License\n\nCopyright (c) 2025 Gabriel Morona Coelho, Matheus da Silva Gastaldi, Jo\u00e3o Carlos Rodrigues Martins\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"sobre/#contato","title":"Contato","text":"<p>Para quest\u00f5es sobre o projeto, voc\u00ea pode:</p> <ul> <li>Abrir uma issue no GitHub</li> <li>Entrar em contato diretamente com os autores via email</li> <li>Participar das discuss\u00f5es na se\u00e7\u00e3o de Discussions do GitHub</li> </ul>"},{"location":"sobre/#agradecimentos","title":"Agradecimentos","text":"<p>Gostar\u00edamos de agradecer \u00e0s seguintes pessoas e organiza\u00e7\u00f5es:</p> <ul> <li>A comunidade de c\u00f3digo aberto do Apache Spark, Delta Lake e Apache Iceberg por desenvolver e manter estas ferramentas incr\u00edveis</li> <li>Todos os contribuidores que ajudaram a melhorar este projeto</li> <li>Institui\u00e7\u00f5es educacionais e empresas que apoiaram o desenvolvimento deste projeto</li> </ul>"}]}