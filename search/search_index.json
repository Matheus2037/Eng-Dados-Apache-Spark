{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Engenharia de Dados com Apache Spark","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o oficial do projeto de Engenharia de Dados com Apache Spark. Este projeto demonstra a implementa\u00e7\u00e3o de solu\u00e7\u00f5es de processamento de dados utilizando tecnologias modernas de big data.</p>"},{"location":"#visao-geral-do-projeto","title":"Vis\u00e3o Geral do Projeto","text":"<p>Este projeto demonstra como utilizar o Apache Spark em conjunto com Delta Lake para criar, manipular e gerenciar dados de forma eficiente, confi\u00e1vel e escal\u00e1vel. A combina\u00e7\u00e3o dessas tecnologias proporciona recursos avan\u00e7ados como:</p> <ul> <li>Transa\u00e7\u00f5es ACID em dados armazenados em data lakes</li> <li>Controle de vers\u00e3o e viagem no tempo (time travel)</li> <li>Upserts e merges eficientes</li> <li>Otimiza\u00e7\u00e3o de esquema e compacta\u00e7\u00e3o de arquivos</li> <li>Integra\u00e7\u00e3o perfeita com o ecossistema Spark</li> </ul> <p>Sobre o Delta Lake</p> <p>O Delta Lake \u00e9 uma camada de armazenamento de c\u00f3digo aberto que traz confiabilidade ao seu data lake. Ele fornece transa\u00e7\u00f5es ACID, manipula\u00e7\u00e3o de metadados escal\u00e1vel e unifica processamento de dados em lote e streaming.</p>"},{"location":"#por-que-usar-apache-spark-com-delta-lake","title":"Por que usar Apache Spark com Delta Lake?","text":"<p>O Apache Spark \u00e9 uma das ferramentas mais populares para processamento de big data, oferecendo:</p> <ul> <li>Velocidade: Execu\u00e7\u00e3o r\u00e1pida de jobs tanto em mem\u00f3ria quanto em disco</li> <li>Facilidade de uso: APIs amig\u00e1veis em Python, Java e Scala</li> <li>Generalidade: Suporte a v\u00e1rias cargas de trabalho como SQL, streaming, machine learning e processamento de grafos</li> <li>Processamento unificado: Uma \u00fanica engine para diversos casos de uso</li> </ul> <p>Quando combinado com o Delta Lake, o Spark ganha recursos adicionais que resolvem muitos desafios comuns em data lakes tradicionais:</p> <ul> <li>Confiabilidade: Transa\u00e7\u00f5es ACID garantem consist\u00eancia de dados mesmo em caso de falhas</li> <li>Qualidade de dados: Restri\u00e7\u00f5es de esquema evitam problemas com dados mal-formados</li> <li>Auditoria: Hist\u00f3rico completo de altera\u00e7\u00f5es nos dados</li> <li>Performance: Otimiza\u00e7\u00f5es espec\u00edficas para consultas e ingest\u00f5es de dados</li> </ul>"},{"location":"#estrutura-da-documentacao","title":"Estrutura da Documenta\u00e7\u00e3o","text":"<p>Esta documenta\u00e7\u00e3o est\u00e1 organizada nas seguintes se\u00e7\u00f5es:</p> <ol> <li>Guia de Instala\u00e7\u00e3o - Instru\u00e7\u00f5es detalhadas para configurar o ambiente</li> <li>Funcionalidades - Descri\u00e7\u00e3o das principais funcionalidades implementadas</li> <li>Exemplos - Exemplos pr\u00e1ticos de uso do projeto</li> <li>Sobre - Informa\u00e7\u00f5es sobre o projeto e seus autores</li> </ol>"},{"location":"#como-comecar","title":"Como Come\u00e7ar","text":"<p>Para come\u00e7ar a utilizar o projeto, siga para o Guia de Instala\u00e7\u00e3o e configure seu ambiente de desenvolvimento.</p>"},{"location":"exemplos/","title":"Exemplos Pr\u00e1ticos","text":"<p>Esta se\u00e7\u00e3o apresenta exemplos pr\u00e1ticos e casos de uso realistas para utiliza\u00e7\u00e3o do Apache Spark com Delta Lake em projetos de engenharia de dados.</p>"},{"location":"exemplos/#workflows-completos-de-processamento-de-dados","title":"Workflows Completos de Processamento de Dados","text":""},{"location":"exemplos/#exemplo-1-pipeline-etl-para-dados-de-vendas","title":"Exemplo 1: Pipeline ETL para Dados de Vendas","text":"<p>Este exemplo demonstra um pipeline ETL (Extract, Transform, Load) completo para processar dados de vendas:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, date_format, sum as spark_sum, when, current_timestamp\nfrom delta.tables import DeltaTable\n\n# Configura\u00e7\u00e3o da sess\u00e3o Spark\nspark = (\n    SparkSession.builder\n    .appName(\"ETL Pipeline de Vendas\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n\n# 1. EXTRA\u00c7\u00c3O - Carregar dados de diferentes fontes\n# Dados de vendas (simulando CSV)\nvendas_df = spark.read.format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .load(\"./dados/vendas.csv\")\n\n# Dados de produtos (simulando JSON)\nprodutos_df = spark.read.format(\"json\") \\\n    .load(\"./dados/produtos.json\")\n\n# Dados de clientes (simulando Delta existente)\nclientes_df = spark.read.format(\"delta\") \\\n    .load(\"./RAW/CLIENTES\")\n\n# 2. TRANSFORMA\u00c7\u00c3O - Processar e enriquecer os dados\n# Limpeza de dados\nvendas_limpas_df = vendas_df \\\n    .dropDuplicates([\"ID_VENDA\"]) \\\n    .filter(col(\"VALOR\") &gt; 0) \\\n    .withColumn(\"DATA\", date_format(col(\"DATA\"), \"yyyy-MM-dd\"))\n\n# Join com dados de produtos\nvendas_com_produtos_df = vendas_limpas_df \\\n    .join(produtos_df, vendas_limpas_df.ID_PRODUTO == produtos_df.ID, \"left\") \\\n    .select(\n        vendas_limpas_df[\"*\"], \n        col(\"NOME_PRODUTO\"), \n        col(\"CATEGORIA\"),\n        col(\"PRECO_UNITARIO\")\n    )\n\n# Join com dados de clientes\nvendas_completas_df = vendas_com_produtos_df \\\n    .join(clientes_df, vendas_com_produtos_df.ID_CLIENTE == clientes_df.ID_CLIENTE, \"left\") \\\n    .select(\n        vendas_com_produtos_df[\"*\"],\n        col(\"NOME_CLIENTE\"),\n        col(\"UF\")\n    )\n\n# Agrega\u00e7\u00f5es para an\u00e1lise\nresumo_por_regiao = vendas_completas_df \\\n    .groupBy(\"UF\") \\\n    .agg(\n        spark_sum(\"VALOR\").alias(\"VALOR_TOTAL\"),\n        spark_sum(when(col(\"CATEGORIA\") == \"Eletr\u00f4nicos\", col(\"VALOR\")).otherwise(0)).alias(\"VALOR_ELETRONICOS\")\n    )\n\n# 3. CARGA - Salvar dados processados em formato Delta\n# Salvar dados detalhados\n(\n    vendas_completas_df\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .partitionBy(\"UF\", \"DATA\")  # Particionar por regi\u00e3o e data\n    .save(\"./SILVER/VENDAS_DETALHADAS\")\n)\n\n# Salvar agrega\u00e7\u00f5es\n(\n    resumo_por_regiao\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .save(\"./GOLD/RESUMO_VENDAS_REGIAO\")\n)\n\n# 4. AUDITORIA - Registrar metadados do processo\naudit_data = [\n    (\n        \"PIPELINE_VENDAS\", \n        current_timestamp(), \n        vendas_df.count(), \n        vendas_completas_df.count(),\n        \"SUCCESS\"\n    )\n]\n\naudit_df = spark.createDataFrame(\n    audit_data, \n    [\"PIPELINE_ID\", \"TIMESTAMP\", \"REGISTROS_ENTRADA\", \"REGISTROS_SAIDA\", \"STATUS\"]\n)\n\n(\n    audit_df\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .save(\"./METADATA/AUDIT_LOGS\")\n)\n\nprint(\"Pipeline ETL de vendas conclu\u00eddo com sucesso!\")\n</code></pre>"},{"location":"exemplos/#exemplo-2-processo-de-incremento-diario-de-dados","title":"Exemplo 2: Processo de Incremento Di\u00e1rio de Dados","text":"<p>Este exemplo demonstra como implementar um processo de carga incremental di\u00e1ria:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, current_date, date_sub\nfrom delta.tables import DeltaTable\n\n# Configura\u00e7\u00e3o da sess\u00e3o Spark\nspark = (\n    SparkSession.builder\n    .appName(\"Carga Incremental\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n\n# Data para filtragem incremental\ndata_atual = current_date()\ndata_inicio = date_sub(data_atual, 1)  # Dados de ontem\n\n# Carregar apenas registros novos da fonte\nnovos_dados = (\n    spark.read.format(\"jdbc\")\n    .option(\"url\", \"jdbc:postgresql://localhost:5432/database\")\n    .option(\"dbtable\", \"vendas\")\n    .option(\"user\", \"usuario\")\n    .option(\"password\", \"senha\")\n    .option(\"driver\", \"org.postgresql.Driver\")\n    .option(\"query\", f\"SELECT * FROM vendas WHERE data_venda &gt;= '{data_inicio}'\")\n    .load()\n)\n\n# Verificar se a tabela Delta j\u00e1 existe\ntry:\n    deltaTable = DeltaTable.forPath(spark, \"./DELTA/VENDAS_HISTORICO\")\n\n    # Executar merge para atualizar registros existentes e inserir novos\n    (\n        deltaTable.alias(\"destino\")\n        .merge(\n            novos_dados.alias(\"origem\"),\n            \"destino.ID_VENDA = origem.ID_VENDA\"\n        )\n        .whenMatchedUpdateAll()\n        .whenNotMatchedInsertAll()\n        .execute()\n    )\n\n    print(f\"Carga incremental conclu\u00edda. {novos_dados.count()} registros processados.\")\n\nexcept:\n    # Se a tabela n\u00e3o existir, criar pela primeira vez\n    (\n        novos_dados\n        .write\n        .format(\"delta\")\n        .mode(\"overwrite\")\n        .save(\"./DELTA/VENDAS_HISTORICO\")\n    )\n\n    print(f\"Primeira carga conclu\u00edda. {novos_dados.count()} registros processados.\")\n\n# Otimizar a tabela ap\u00f3s a carga\nspark.sql(\"OPTIMIZE delta.`./DELTA/VENDAS_HISTORICO`\")\n</code></pre>"},{"location":"exemplos/#casos-de-uso-comuns-em-engenharia-de-dados","title":"Casos de Uso Comuns em Engenharia de Dados","text":""},{"location":"exemplos/#caso-1-processamento-de-dados-de-sensores-iot","title":"Caso 1: Processamento de Dados de Sensores IoT","text":"<p>Este caso de uso demonstra como processar dados de sensores IoT que chegam em lotes frequentes:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, from_json, window, avg, max as spark_max, min as spark_min\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\nfrom delta.tables import DeltaTable\n\n# Configura\u00e7\u00e3o da sess\u00e3o Spark\nspark = (\n    SparkSession.builder\n    .appName(\"Processamento IoT\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n\n# Definir schema dos dados de sensores\nschema_sensores = StructType([\n    StructField(\"device_id\", StringType(), False),\n    StructField(\"timestamp\", TimestampType(), False),\n    StructField(\"temperatura\", FloatType(), True),\n    StructField(\"umidade\", FloatType(), True),\n    StructField(\"pressao\", FloatType(), True),\n    StructField(\"localizacao\", StringType(), True)\n])\n\n# Carregar lote de dados de sensores\ndados_sensores = (\n    spark.read.format(\"json\")\n    .schema(schema_sensores)\n    .load(\"./dados/sensores_lote_atual.json\")\n)\n\n# Processar dados - calcular m\u00e9dias por janela de tempo e dispositivo\nmetricas_por_dispositivo = (\n    dados_sensores\n    .groupBy(\n        \"device_id\",\n        window(col(\"timestamp\"), \"1 hour\")\n    )\n    .agg(\n        avg(\"temperatura\").alias(\"temp_media\"),\n        spark_max(\"temperatura\").alias(\"temp_max\"),\n        spark_min(\"temperatura\").alias(\"temp_min\"),\n        avg(\"umidade\").alias(\"umidade_media\"),\n        avg(\"pressao\").alias(\"pressao_media\")\n    )\n)\n\n# Tabela delta para armazenar dados raw\n(\n    dados_sensores\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .partitionBy(\"device_id\")\n    .save(\"./IOT/RAW_SENSORES\")\n)\n\n# Tabela delta para armazenar m\u00e9tricas agregadas\n(\n    metricas_por_dispositivo\n    .write\n    .format(\"delta\")\n    .mode(\"append\")\n    .partitionBy(\"device_id\")\n    .save(\"./IOT/METRICAS_SENSORES\")\n)\n\n# Detectar anomalias (exemplo: temperatura muito alta)\nanomalias = (\n    dados_sensores\n    .filter(col(\"temperatura\") &gt; 90.0)\n    .select(\"device_id\", \"timestamp\", \"temperatura\", \"localizacao\")\n)\n\nif anomalias.count() &gt; 0:\n    (\n        anomalias\n        .write\n        .format(\"delta\")\n        .mode(\"append\")\n        .save(\"./IOT/ANOMALIAS\")\n    )\n    print(f\"Detectadas {anomalias.count()} anomalias de temperatura!\")\n</code></pre>"},{"location":"exemplos/#caso-2-data-warehouse-incremental","title":"Caso 2: Data Warehouse Incremental","text":"<p>Este exemplo demonstra como construir e manter um data warehouse usando o padr\u00e3o SCD (Slowly Changing Dimension) Tipo 2 com Delta Lake:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, current_timestamp, lit\nfrom delta.tables import DeltaTable\n\n# Configura\u00e7\u00e3o da sess\u00e3o Spark\nspark = (\n    SparkSession.builder\n    .appName(\"Data Warehouse SCD2\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n\n# Carregar novos dados da dimens\u00e3o clientes\nnovos_clientes = (\n    spark.read.format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(\"./dados/novos_clientes.csv\")\n    .withColumn(\"data_atualizacao\", current_timestamp())\n    .withColumn(\"ativo\", lit(True))\n)\n\n# Implementar SCD Tipo 2 (mant\u00e9m hist\u00f3rico de mudan\u00e7as)\ntry:\n    # Verificar se tabela dimens\u00e3o j\u00e1 existe\n    dim_clientes = DeltaTable.forPath(spark, \"./DW/DIM_CLIENTES\")\n\n    # Identificar registros novos e alterados\n    # Atualiza\u00e7\u00f5es s\u00e3o identificadas por chave igual, mas valores diferentes\n    atualizado_df = dim_clientes.toDF().alias(\"atual\").join(\n        novos_clientes.alias(\"novo\"),\n        \"ID_CLIENTE\"\n    ).where(\n        \"\"\"\n        atual.ativo = true AND\n        (\n            atual.NOME_CLIENTE != novo.NOME_CLIENTE OR\n            atual.UF != novo.UF OR\n            atual.LIMITE_CREDITO != novo.LIMITE_CREDITO\n        )\n        \"\"\"\n    ).select(\"atual.ID_CLIENTE\")\n\n    # Expirar registros atuais (marcar como inativos)\n    (\n        dim_clientes.alias(\"atual\")\n        .merge(\n            atualizado_df.alias(\"atualizado\"),\n            \"atual.ID_CLIENTE = atualizado.ID_CLIENTE AND atual.ativo = true\"\n        )\n        .whenMatched()\n        .updateExpr({\"ativo\": \"false\", \"data_fim\": \"current_timestamp()\"})\n        .execute()\n    )\n\n    # Inserir todos os novos registros\n    (\n        novos_clientes\n        .write\n        .format(\"delta\")\n        .mode(\"append\")\n        .save(\"./DW/DIM_CLIENTES\")\n    )\n\nexcept Exception as e:\n    # Se a tabela n\u00e3o existir, criar pela primeira vez\n    # Adicionar colunas de controle para SCD Tipo 2\n    (\n        novos_clientes\n        .withColumn(\"data_inicio\", current_timestamp())\n        .withColumn(\"data_fim\", lit(None).cast(\"timestamp\"))\n        .write\n        .format(\"delta\")\n        .mode(\"overwrite\")\n        .save(\"./DW/DIM_CLIENTES\")\n    )\n\n# Otimizar a tabela ap\u00f3s as opera\u00e7\u00f5es\nspark.sql(\"OPTIMIZE delta.`./DW/DIM_CLIENTES`\")\n</code></pre>"},{"location":"exemplos/#integracao-com-diferentes-fontes-de-dados","title":"Integra\u00e7\u00e3o com Diferentes Fontes de Dados","text":""},{"location":"exemplos/#exemplo-1-integracao-com-bancos-de-dados-relacionais","title":"Exemplo 1: Integra\u00e7\u00e3o com Bancos de Dados Relacionais","text":"<p>Este exemplo demonstra como integrar dados de um banco PostgreSQL com Delta Lake:</p> <p>```python from pyspark.sql import SparkSession from delta.tables import DeltaTable</p>"},{"location":"exemplos/#configuracao-da-sessao-spark","title":"Configura\u00e7\u00e3o da sess\u00e3o Spark","text":"<p>spark = (     SparkSession.builder     .appName(\"Integra\u00e7\u00e3o PostgreSQL\")     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:42.3.1\")     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")     .getOrCreate() )</p>"},{"location":"exemplos/#carregar-dados-do-postgresql","title":"Carregar dados do PostgreSQL","text":"<p>jdbc_params = {     \"url\": \"jdbc:postgresql://localhost:5432/meu_banco\",</p>"},{"location":"funcionalidades/","title":"Funcionalidades","text":"<p>Este documento detalha as principais funcionalidades implementadas no projeto de Engenharia de Dados com Apache Spark, com foco especial nas opera\u00e7\u00f5es utilizando Delta Lake.</p>"},{"location":"funcionalidades/#visao-geral-das-operacoes-com-delta-lake","title":"Vis\u00e3o Geral das Opera\u00e7\u00f5es com Delta Lake","text":"<p>O Delta Lake \u00e9 uma camada de armazenamento que adiciona recursos essenciais aos data lakes, trazendo confiabilidade e performance para opera\u00e7\u00f5es de dados em larga escala. Este projeto demonstra a implementa\u00e7\u00e3o das seguintes capacidades:</p> <ul> <li>ACID Transactions: Garantia de atomicidade, consist\u00eancia, isolamento e durabilidade para todas as opera\u00e7\u00f5es de dados</li> <li>Schema Enforcement: Preven\u00e7\u00e3o autom\u00e1tica contra inser\u00e7\u00e3o de dados inv\u00e1lidos</li> <li>Schema Evolution: Modifica\u00e7\u00e3o flex\u00edvel do esquema ao longo do tempo</li> <li>Time Travel: Acesso a vers\u00f5es anteriores dos dados</li> <li>CRUD Operations: Suporte completo a Create, Read, Update e Delete </li> <li>Merge Operations: Opera\u00e7\u00f5es de UPSERT eficientes (update + insert)</li> <li>Data Optimization: Compacta\u00e7\u00e3o de arquivos e otimiza\u00e7\u00e3o de consultas</li> </ul>"},{"location":"funcionalidades/#operacoes-crud-com-delta-lake","title":"Opera\u00e7\u00f5es CRUD com Delta Lake","text":""},{"location":"funcionalidades/#1-criacao-de-tabelas-delta-create","title":"1. Cria\u00e7\u00e3o de Tabelas Delta (Create)","text":"<p>O Delta Lake permite criar tabelas a partir de DataFrames Spark com sintaxe simples, adicionando garantias de transa\u00e7\u00e3o e capacidades avan\u00e7adas.</p>"},{"location":"funcionalidades/#exemplo-de-criacao","title":"Exemplo de Cria\u00e7\u00e3o:","text":"<pre><code># Importa\u00e7\u00f5es necess\u00e1rias\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType\nfrom delta import *\n\n# Configura\u00e7\u00e3o da sess\u00e3o Spark com suporte ao Delta Lake\nspark = (\n    SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .getOrCreate()\n)\n\n# Dados de exemplo\ndata = [\n    (\"ID001\", \"CLIENTE_X\", \"SP\", \"ATIVO\",   250000.00),\n    (\"ID002\", \"CLIENTE_Y\", \"SC\", \"INATIVO\", 400000.00),\n    (\"ID003\", \"CLIENTE_Z\", \"DF\", \"ATIVO\",   1000000.00)\n]\n\n# Defini\u00e7\u00e3o do schema\nschema = (\n    StructType([\n        StructField(\"ID_CLIENTE\",     StringType(), True),\n        StructField(\"NOME_CLIENTE\",   StringType(), True),\n        StructField(\"UF\",             StringType(), True),\n        StructField(\"STATUS\",         StringType(), True),\n        StructField(\"LIMITE_CREDITO\", FloatType(),  True)\n    ])\n)\n\n# Cria\u00e7\u00e3o do DataFrame\ndf = spark.createDataFrame(data=data, schema=schema)\n\n# Escrita no formato Delta\n(\n    df\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")  # Op\u00e7\u00f5es: append, overwrite, ignore, errorIfExists\n    .save(\"./RAW/CLIENTES\")\n)\n</code></pre> <p>Modos de Escrita</p> <p>O Delta Lake suporta diferentes modos de escrita: - overwrite: Substitui dados existentes - append: Adiciona novos dados sem modificar existentes - ignore: N\u00e3o faz nada se o destino j\u00e1 existir - errorIfExists: Falha se o destino j\u00e1 existir</p>"},{"location":"funcionalidades/#2-leitura-de-tabelas-delta-read","title":"2. Leitura de Tabelas Delta (Read)","text":"<p>A leitura de tabelas Delta \u00e9 feita com a mesma API do Spark, mas com benef\u00edcios adicionais como consist\u00eancia de leitura e capacidade de acessar vers\u00f5es anteriores.</p>"},{"location":"funcionalidades/#exemplo-de-leitura","title":"Exemplo de Leitura:","text":"<pre><code># Leitura simples de tabela Delta\ndf_clientes = (\n    spark\n    .read\n    .format(\"delta\")\n    .load(\"./RAW/CLIENTES\")\n)\n\n# Exibir resultados\ndf_clientes.show()\n</code></pre>"},{"location":"funcionalidades/#leitura-de-versao-especifica-time-travel","title":"Leitura de Vers\u00e3o Espec\u00edfica (Time Travel):","text":"<pre><code># Ler vers\u00e3o espec\u00edfica da tabela\ndf_clientes_versao_anterior = (\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"versionAsOf\", 0)  # Vers\u00e3o 0 (inicial)\n    .load(\"./RAW/CLIENTES\")\n)\n\n# Alternativa usando timestamp\ndf_clientes_timestamp = (\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"timestampAsOf\", \"2023-01-01 00:00:00\")\n    .load(\"./RAW/CLIENTES\")\n)\n</code></pre>"},{"location":"funcionalidades/#3-atualizacao-de-dados-update","title":"3. Atualiza\u00e7\u00e3o de Dados (Update)","text":"<p>O Delta Lake suporta atualiza\u00e7\u00f5es de dados de forma eficiente, permitindo modificar registros existentes sem reescrever toda a tabela.</p>"},{"location":"funcionalidades/#exemplo-de-atualizacao","title":"Exemplo de Atualiza\u00e7\u00e3o:","text":"<pre><code>from delta.tables import DeltaTable\n\n# Carregar tabela Delta para opera\u00e7\u00f5es\ndeltaTable = DeltaTable.forPath(spark, \"./RAW/CLIENTES\")\n\n# Atualizar registros\n(\n    deltaTable.update(\n        condition=\"STATUS = 'ATIVO'\",\n        set={\"LIMITE_CREDITO\": \"LIMITE_CREDITO * 1.1\"}  # Aumento de 10% no limite\n    )\n)\n</code></pre>"},{"location":"funcionalidades/#4-exclusao-de-dados-delete","title":"4. Exclus\u00e3o de Dados (Delete)","text":"<p>Opera\u00e7\u00f5es de exclus\u00e3o permitem remover registros que atendem a determinadas condi\u00e7\u00f5es.</p>"},{"location":"funcionalidades/#exemplo-de-exclusao","title":"Exemplo de Exclus\u00e3o:","text":"<pre><code># Deletar registros com condi\u00e7\u00e3o\ndeltaTable.delete(\"LIMITE_CREDITO &lt; 400000.0\")\n</code></pre>"},{"location":"funcionalidades/#merge-operations-upsert","title":"Merge Operations (UPSERT)","text":"<p>Uma das opera\u00e7\u00f5es mais poderosas do Delta Lake \u00e9 o <code>merge</code>, que permite combinar opera\u00e7\u00f5es de atualiza\u00e7\u00e3o e inser\u00e7\u00e3o em uma \u00fanica transa\u00e7\u00e3o at\u00f4mica.</p>"},{"location":"funcionalidades/#exemplo-de-merge","title":"Exemplo de MERGE:","text":"<pre><code># Novos dados para merge\nnew_data = [\n    (\"ID001\", \"CLIENTE_X\", \"SP\", \"INATIVO\", 0.00),         # Update - cliente existente\n    (\"ID002\", \"CLIENTE_Y\", \"SC\", \"ATIVO\",   400000.00),    # Update - cliente existente\n    (\"ID004\", \"CLIENTE_Z\", \"DF\", \"ATIVO\",   5000000.00)    # Insert - novo cliente\n]\n\n# Criar DataFrame com novos dados\ndf_new = spark.createDataFrame(data=new_data, schema=schema)\n\n# Carregar tabela Delta\ndeltaTable = DeltaTable.forPath(spark, \"./RAW/CLIENTES\")\n\n# Executar opera\u00e7\u00e3o de merge\n(\n    deltaTable.alias(\"dados_atuais\")\n    .merge(\n        df_new.alias(\"novos_dados\"),\n        \"dados_atuais.ID_CLIENTE = novos_dados.ID_CLIENTE\"  # Condi\u00e7\u00e3o de merge\n    )\n    .whenMatchedUpdateAll()      # Atualiza todos os campos quando encontra correspond\u00eancia\n    .whenNotMatchedInsertAll()   # Insere novo registro quando n\u00e3o encontra correspond\u00eancia\n    .execute()\n)\n</code></pre> <p>Controle Granular</p> <p>Voc\u00ea tamb\u00e9m pode ter controle mais granular usando: <pre><code>.whenMatched(condition=\"novos_dados.LIMITE_CREDITO &gt; 0\")\n.updateExpr({\"STATUS\": \"novos_dados.STATUS\", \"LIMITE_CREDITO\": \"novos_dados.LIMITE_CREDITO\"})\n.whenNotMatched(condition=\"novos_dados.LIMITE_CREDITO &gt; 1000000\")\n.insertExpr({\n    \"ID_CLIENTE\": \"novos_dados.ID_CLIENTE\",\n    \"NOME_CLIENTE\": \"novos_dados.NOME_CLIENTE\",\n    \"UF\": \"novos_dados.UF\",\n    \"STATUS\": \"novos_dados.STATUS\",\n    \"LIMITE_CREDITO\": \"novos_dados.LIMITE_CREDITO\"\n})\n</code></pre></p>"},{"location":"funcionalidades/#recursos-avancados","title":"Recursos Avan\u00e7ados","text":""},{"location":"funcionalidades/#1-time-travel-viagem-no-tempo","title":"1. Time Travel (Viagem no Tempo)","text":"<p>O Delta Lake mant\u00e9m um hist\u00f3rico de todas as modifica\u00e7\u00f5es, permitindo acessar e consultar vers\u00f5es anteriores dos dados.</p>"},{"location":"funcionalidades/#exemplos-de-time-travel","title":"Exemplos de Time Travel:","text":"<pre><code># Listar hist\u00f3rico de vers\u00f5es\ndeltaTable.history().show()\n\n# Ler vers\u00e3o espec\u00edfica\ndf_versao_1 = (\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"versionAsOf\", 1)\n    .load(\"./RAW/CLIENTES\")\n)\n\n# Restaurar para vers\u00e3o anterior\n(\n    spark\n    .read\n    .format(\"delta\")\n    .option(\"versionAsOf\", 1)\n    .load(\"./RAW/CLIENTES\")\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .save(\"./RAW/CLIENTES\")\n)\n</code></pre>"},{"location":"funcionalidades/#2-otimizacoes-de-dados","title":"2. Otimiza\u00e7\u00f5es de Dados","text":"<p>O Delta Lake oferece comandos para otimizar o layout dos arquivos e melhorar o desempenho das consultas.</p>"},{"location":"funcionalidades/#compactacao-de-arquivos","title":"Compacta\u00e7\u00e3o de Arquivos:","text":"<pre><code># Compactar arquivos pequenos\nspark.sql(f\"OPTIMIZE delta.`./RAW/CLIENTES`\")\n\n# Compactar e ordenar por coluna espec\u00edfica (Z-Order)\nspark.sql(f\"OPTIMIZE delta.`./RAW/CLIENTES` ZORDER BY (UF, STATUS)\")\n</code></pre>"},{"location":"funcionalidades/#vacuum-limpeza-de-arquivos-antigos","title":"Vacuum (Limpeza de Arquivos Antigos):","text":"<pre><code># Remover arquivos antigos que n\u00e3o s\u00e3o mais necess\u00e1rios para time travel\n# Mantendo apenas 7 dias de hist\u00f3rico\ndeltaTable.vacuum(retentionHours=24*7)\n</code></pre> <p>Cuidado com Vacuum</p> <p>O comando <code>vacuum</code> remove permanentemente arquivos antigos, limitando o time travel. Por padr\u00e3o, o Delta Lake impede a remo\u00e7\u00e3o de arquivos com menos de 7 dias para evitar problemas com consultas ativas.</p>"},{"location":"funcionalidades/#3-schema-evolution-evolucao-de-esquema","title":"3. Schema Evolution (Evolu\u00e7\u00e3o de Esquema)","text":"<p>O Delta Lake permite adicionar, remover ou modificar colunas de maneira controlada.</p>"},{"location":"funcionalidades/#exemplo-de-evolucao-de-esquema","title":"Exemplo de Evolu\u00e7\u00e3o de Esquema:","text":"<pre><code># Adicionar nova coluna ao esquema\nfrom pyspark.sql.functions import lit\n\n# Ler dados existentes\ndf_atual = spark.read.format(\"delta\").load(\"./RAW/CLIENTES\")\n\n# Adicionar nova coluna\ndf_com_nova_coluna = df_atual.withColumn(\"DATA_ATUALIZACAO\", lit(\"2024-04-23\"))\n\n# Escrever de volta com esquema atualizado\n(\n    df_com_nova_coluna\n    .write\n    .format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"mergeSchema\", \"true\")  # Habilita evolu\u00e7\u00e3o de esquema\n    .save(\"./RAW/CLIENTES\")\n)\n</code></pre>"},{"location":"funcionalidades/#integracao-com-sql","title":"Integra\u00e7\u00e3o com SQL","text":"<p>O Delta Lake se integra perfeitamente com SQL, permitindo usar sintaxe familiar para opera\u00e7\u00f5es:</p> <pre><code># Registrar tabela para uso com SQL\nspark.sql(f\"CREATE TABLE clientes USING DELTA LOCATION './RAW/CLIENTES'\")\n\n# Consultar dados com SQL\nresultado = spark.sql(\"SELECT * FROM clientes WHERE UF = 'SP'\")\nresultado.show()\n\n# Atualizar com SQL\nspark.sql(\"UPDATE clientes SET LIMITE_CREDITO = 300000 WHERE ID_CLIENTE = 'ID001'\")\n\n# Merge com SQL\nspark.sql(\"\"\"\nMERGE INTO clientes AS target\nUSING updates AS source\nON target.ID_CLIENTE = source.ID_CLIENTE\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n\"\"\")\n</code></pre>"},{"location":"funcionalidades/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Agora que voc\u00ea conhece as principais funcionalidades do Delta Lake implementadas neste projeto, explore a se\u00e7\u00e3o de Exemplos para ver casos de uso completos e aplica\u00e7\u00f5es pr\u00e1ticas destas funcionalidades.</p>"},{"location":"guia-instalacao/","title":"Guia de Instala\u00e7\u00e3o","text":"<p>Este guia fornece instru\u00e7\u00f5es detalhadas para configurar seu ambiente de desenvolvimento e instalar todas as depend\u00eancias necess\u00e1rias para trabalhar com o projeto de Engenharia de Dados com Apache Spark.</p>"},{"location":"guia-instalacao/#requisitos-do-sistema","title":"Requisitos do Sistema","text":"<p>Antes de iniciar a instala\u00e7\u00e3o, certifique-se de que seu sistema atende aos seguintes requisitos:</p>"},{"location":"guia-instalacao/#requisitos-de-hardware","title":"Requisitos de Hardware","text":"<ul> <li>Processador: M\u00ednimo de 2 cores (recomendado 4+ cores)</li> <li>Mem\u00f3ria RAM: M\u00ednimo de 4GB (recomendado 8GB+ para cargas de trabalho maiores)</li> <li>Espa\u00e7o em Disco: M\u00ednimo de 2GB dispon\u00edveis</li> </ul>"},{"location":"guia-instalacao/#pre-requisitos-de-software","title":"Pr\u00e9-requisitos de Software","text":"<ul> <li>Sistema Operacional: Windows 10/11, macOS (10.15+), ou Linux (Ubuntu 20.04+, CentOS 7+)</li> <li>Python: Vers\u00e3o 3.9.9</li> <li>Java: JDK 8 ou superior (necess\u00e1rio para Apache Spark)</li> <li>Poetry: Ferramenta de gerenciamento de depend\u00eancias para Python</li> </ul> <p>Importante sobre Java</p> <p>O Apache Spark requer Java para funcionar. Certifique-se de ter o JDK instalado e a vari\u00e1vel de ambiente <code>JAVA_HOME</code> configurada corretamente.</p>"},{"location":"guia-instalacao/#instalacao-do-python","title":"Instala\u00e7\u00e3o do Python","text":"<p>Se voc\u00ea ainda n\u00e3o tem Python 3.9.9 instalado:</p>"},{"location":"guia-instalacao/#para-windows","title":"Para Windows","text":"<ol> <li>Baixe o instalador do Python em python.org</li> <li>Execute o instalador e marque a op\u00e7\u00e3o \"Add Python to PATH\"</li> <li>Conclua a instala\u00e7\u00e3o seguindo as instru\u00e7\u00f5es na tela</li> </ol>"},{"location":"guia-instalacao/#para-macos","title":"Para macOS","text":"<p>Utilizando o Homebrew: <pre><code>brew install python@3.9\n</code></pre></p>"},{"location":"guia-instalacao/#para-linux-ubuntudebian","title":"Para Linux (Ubuntu/Debian)","text":"<pre><code>sudo apt update\nsudo apt install python3.9 python3.9-venv python3.9-dev\n</code></pre>"},{"location":"guia-instalacao/#instalacao-do-poetry","title":"Instala\u00e7\u00e3o do Poetry","text":"<p>O Poetry \u00e9 a ferramenta recomendada para gerenciar depend\u00eancias e ambientes virtuais neste projeto.</p>"},{"location":"guia-instalacao/#para-windows-macos-e-linux","title":"Para Windows, macOS e Linux","text":"<pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Ap\u00f3s a instala\u00e7\u00e3o, adicione o Poetry ao seu PATH seguindo as instru\u00e7\u00f5es exibidas no final da instala\u00e7\u00e3o.</p> <p>Para verificar a instala\u00e7\u00e3o, execute: <pre><code>poetry --version\n</code></pre></p>"},{"location":"guia-instalacao/#clonando-o-repositorio","title":"Clonando o Reposit\u00f3rio","text":"<p>Clone o reposit\u00f3rio do projeto utilizando Git:</p> <pre><code>git clone https://github.com/gabrielcoelho/Eng-Dados-Apache-Spark.git\ncd Eng-Dados-Apache-Spark\n</code></pre>"},{"location":"guia-instalacao/#instalando-dependencias-com-poetry","title":"Instalando Depend\u00eancias com Poetry","text":"<p>Uma vez dentro do diret\u00f3rio do projeto, instale todas as depend\u00eancias:</p> <pre><code>poetry install\n</code></pre> <p>Este comando criar\u00e1 um ambiente virtual e instalar\u00e1 as seguintes depend\u00eancias principais: - pyspark (3.4.2): Motor de processamento distribu\u00eddo - delta-spark (2.4.0): Extens\u00e3o do Spark para Delta Lake - jupyterlab (4.4.0+): Ambiente interativo para execu\u00e7\u00e3o de notebooks</p>"},{"location":"guia-instalacao/#ativando-o-ambiente-virtual","title":"Ativando o Ambiente Virtual","text":"<p>Para ativar o ambiente virtual criado pelo Poetry:</p> <pre><code>poetry shell\n</code></pre> <p>Ou voc\u00ea pode executar comandos diretamente dentro do ambiente virtual sem ativ\u00e1-lo permanentemente:</p> <pre><code>poetry run jupyter lab\n</code></pre>"},{"location":"guia-instalacao/#verificando-a-instalacao","title":"Verificando a Instala\u00e7\u00e3o","text":"<p>Para verificar se tudo foi instalado corretamente:</p> <ol> <li>Ative o ambiente virtual: <code>poetry shell</code></li> <li>Abra o Python REPL: <code>python</code></li> <li>Tente importar as bibliotecas principais:</li> </ol> <pre><code>import pyspark\nfrom delta import *\n\nprint(f\"PySpark vers\u00e3o: {pyspark.__version__}\")\nprint(\"Delta Lake dispon\u00edvel!\")\n</code></pre> <p>Se n\u00e3o ocorrer nenhum erro, sua instala\u00e7\u00e3o est\u00e1 funcionando corretamente.</p>"},{"location":"guia-instalacao/#iniciando-o-jupyterlab","title":"Iniciando o JupyterLab","text":"<p>Para iniciar o JupyterLab e acessar os notebooks do projeto:</p> <pre><code>poetry run jupyter lab\n</code></pre> <p>Isso abrir\u00e1 o JupyterLab no seu navegador padr\u00e3o. Navegue at\u00e9 a pasta <code>pyspark-delta</code> para acessar os notebooks de exemplo.</p>"},{"location":"guia-instalacao/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guia-instalacao/#problema-erro-ao-instalar-o-pyspark","title":"Problema: Erro ao instalar o PySpark","text":"<p>Sintoma: Erros relacionados a depend\u00eancias do Java durante a instala\u00e7\u00e3o do PySpark.</p> <p>Solu\u00e7\u00e3o: Certifique-se de que o Java (JDK 8+) est\u00e1 instalado e configurado: <pre><code># Verifique a instala\u00e7\u00e3o do Java\njava -version\n\n# Configure a vari\u00e1vel JAVA_HOME se necess\u00e1rio\nexport JAVA_HOME=/caminho/para/seu/jdk\n</code></pre></p>"},{"location":"guia-instalacao/#problema-dependencias-nao-encontradas-apos-instalacao","title":"Problema: Depend\u00eancias n\u00e3o encontradas ap\u00f3s instala\u00e7\u00e3o","text":"<p>Sintoma: M\u00f3dulos n\u00e3o encontrados mesmo ap\u00f3s instala\u00e7\u00e3o com Poetry.</p> <p>Solu\u00e7\u00e3o: Certifique-se de que est\u00e1 usando o ambiente virtual correto: <pre><code># Verifique se o ambiente virtual est\u00e1 ativo\npoetry env info\n\n# Ative o ambiente se necess\u00e1rio\npoetry shell\n</code></pre></p>"},{"location":"guia-instalacao/#problema-o-jupyterlab-nao-consegue-encontrar-dependencias","title":"Problema: O JupyterLab n\u00e3o consegue encontrar depend\u00eancias","text":"<p>Sintoma: Erros de importa\u00e7\u00e3o ao executar c\u00e9lulas no JupyterLab.</p> <p>Solu\u00e7\u00e3o: Certifique-se de que o Jupyter est\u00e1 usando o kernel correto: 1. No JupyterLab, verifique no canto superior direito qual kernel est\u00e1 sendo usado 2. Selecione o kernel correspondente ao seu ambiente Poetry 3. Se o kernel n\u00e3o estiver dispon\u00edvel, instale o ipykernel no ambiente:    <pre><code>poetry run pip install ipykernel\npoetry run python -m ipykernel install --user --name=eng-dados-spark\n</code></pre></p>"},{"location":"guia-instalacao/#problema-erros-relacionados-ao-delta-lake","title":"Problema: Erros relacionados ao Delta Lake","text":"<p>Sintoma: <code>ClassNotFoundException</code> ou erros similares ao tentar usar funcionalidades do Delta Lake.</p> <p>Solu\u00e7\u00e3o: Verifique se as depend\u00eancias do Delta Lake est\u00e3o corretamente configuradas: <pre><code>from pyspark.sql import SparkSession\n\n# Configure a sess\u00e3o Spark com suporte ao Delta Lake\nspark = SparkSession.builder \\\n    .appName(\"DeltaLakeTest\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n</code></pre></p>"},{"location":"guia-instalacao/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Ap\u00f3s completar a instala\u00e7\u00e3o, voc\u00ea est\u00e1 pronto para explorar as funcionalidades do projeto e executar os exemplos dispon\u00edveis.</p>"},{"location":"sobre/","title":"Sobre o Projeto","text":""},{"location":"sobre/#o-projeto","title":"O Projeto","text":"<p>O projeto Engenharia de Dados com Apache Spark foi desenvolvido para demonstrar a implementa\u00e7\u00e3o de solu\u00e7\u00f5es modernas de processamento de dados utilizando Apache Spark e Delta Lake. Este projeto serve como uma refer\u00eancia pr\u00e1tica para engenheiros de dados e cientistas de dados que desejam aprender e aplicar t\u00e9cnicas de processamento de big data com garantias transacionais.</p>"},{"location":"sobre/#objetivos-do-projeto","title":"Objetivos do Projeto","text":"<ul> <li>Demonstrar as capacidades do Apache Spark para processamento de dados em larga escala</li> <li>Implementar opera\u00e7\u00f5es CRUD completas utilizando Delta Lake</li> <li>Fornecer exemplos pr\u00e1ticos de pipelines de dados e casos de uso reais</li> <li>Servir como material educativo e de refer\u00eancia para profissionais de dados</li> </ul>"},{"location":"sobre/#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Apache Spark: Framework de processamento distribu\u00eddo de c\u00f3digo aberto</li> <li>Delta Lake: Camada de armazenamento para confiabilidade em data lakes</li> <li>Python: Linguagem de programa\u00e7\u00e3o principal para implementa\u00e7\u00e3o</li> <li>JupyterLab: Ambiente interativo para explora\u00e7\u00e3o e documenta\u00e7\u00e3o</li> <li>Poetry: Gerenciamento de depend\u00eancias do projeto</li> </ul>"},{"location":"sobre/#equipe","title":"Equipe","text":"<p>O projeto foi desenvolvido por uma equipe de engenheiros de dados comprometidos com a excel\u00eancia t\u00e9cnica e o compartilhamento de conhecimento:</p>"},{"location":"sobre/#autores","title":"Autores","text":"Nome Email Papel Matheus da Silva Gastaldi matheusdasilvagastaldi@gmail.com Desenvolvedor Principal Gabriel Morona Coelho gabrielmorona0229@gmail.com Arquiteto de Dados Jo\u00e3o Carlos Rodrigues Martins joaocarlosrm2004@gmail.com Engenheiro de Dados"},{"location":"sobre/#como-contribuir","title":"Como Contribuir","text":"<p>N\u00f3s encorajamos e valorizamos contribui\u00e7\u00f5es da comunidade! Se voc\u00ea deseja contribuir com este projeto, aqui est\u00e3o algumas maneiras de come\u00e7ar:</p>"},{"location":"sobre/#reportando-problemas","title":"Reportando Problemas","text":"<p>Se voc\u00ea encontrar bugs ou tiver sugest\u00f5es de melhorias:</p> <ol> <li>Verifique se o problema j\u00e1 n\u00e3o foi reportado na se\u00e7\u00e3o de Issues do GitHub</li> <li>Abra uma nova issue com um t\u00edtulo claro e descri\u00e7\u00e3o detalhada</li> <li>Inclua passos para reproduzir o problema, comportamento esperado vs. atual, e capturas de tela se necess\u00e1rio</li> </ol>"},{"location":"sobre/#enviando-pull-requests","title":"Enviando Pull Requests","text":"<p>Para contribuir com c\u00f3digo:</p> <ol> <li>Fork este reposit\u00f3rio</li> <li>Crie uma branch para sua feature (<code>git checkout -b feature/nova-funcionalidade</code>)</li> <li>Fa\u00e7a commit das suas mudan\u00e7as (<code>git commit -m 'Adiciona nova funcionalidade'</code>)</li> <li>Push para a branch (<code>git push origin feature/nova-funcionalidade</code>)</li> <li>Abra um Pull Request</li> </ol>"},{"location":"sobre/#diretrizes-de-codigo","title":"Diretrizes de C\u00f3digo","text":"<ul> <li>Siga o estilo de c\u00f3digo PEP 8 para Python</li> <li>Escreva testes para novas funcionalidades</li> <li>Atualize a documenta\u00e7\u00e3o quando necess\u00e1rio</li> <li>Mantenha os commits organizados e com mensagens claras</li> </ul>"},{"location":"sobre/#licenca","title":"Licen\u00e7a","text":"<p>Este projeto \u00e9 disponibilizado sob a licen\u00e7a MIT. Veja o arquivo <code>LICENSE</code> para mais detalhes.</p> <pre><code>MIT License\n\nCopyright (c) 2025 Gabriel Morona Coelho, Matheus da Silva Gastaldi, Jo\u00e3o Carlos Rodrigues Martins\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"sobre/#contato","title":"Contato","text":"<p>Para quest\u00f5es sobre o projeto, voc\u00ea pode:</p> <ul> <li>Abrir uma issue no GitHub</li> <li>Entrar em contato diretamente com os autores via email</li> <li>Participar das discuss\u00f5es na se\u00e7\u00e3o de Discussions do GitHub</li> </ul>"},{"location":"sobre/#agradecimentos","title":"Agradecimentos","text":"<p>Gostar\u00edamos de agradecer \u00e0s seguintes pessoas e organiza\u00e7\u00f5es:</p> <ul> <li>A comunidade de c\u00f3digo aberto do Apache Spark e Delta Lake por desenvolver e manter estas ferramentas incr\u00edveis</li> <li>Todos os contribuidores que ajudaram a melhorar este projeto</li> <li>Institui\u00e7\u00f5es educacionais e empresas que apoiaram o desenvolvimento deste projeto</li> </ul>"},{"location":"sobre/#historico-do-projeto","title":"Hist\u00f3rico do Projeto","text":"<ul> <li>Abril 2025: Lan\u00e7amento da documenta\u00e7\u00e3o completa</li> <li>Mar\u00e7o 2025: Adi\u00e7\u00e3o de novos exemplos de casos de uso</li> <li>Fevereiro 2025: Integra\u00e7\u00e3o com Delta Lake</li> <li>Janeiro 2025: In\u00edcio do projeto com foco no Apache Spark</li> </ul>"}]}